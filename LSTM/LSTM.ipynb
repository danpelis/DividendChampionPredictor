{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aaffc64-9c71-45ab-8566-88b46cbc8541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Notebook\n",
    "##### Perform Regression on Dividend Data using an LSTM based network\n",
    "##### Aim to predict next value in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-stadium",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SGD' from 'keras.optimizers' (c:\\users\\jlvbe\\miniconda3\\lib\\site-packages\\keras\\optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20468/347708475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SGD' from 'keras.optimizers' (c:\\users\\jlvbe\\miniconda3\\lib\\site-packages\\keras\\optimizers.py)"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional, InputLayer\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a12ac-5fed-4550-a842-4709a14fcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ticker data from GetTickers.ipynb\n",
    "kept_status = np.load(\"../data/numpy/kept_status.npy\")\n",
    "lost_status = np.load(\"../data/numpy/lost_status.npy\")\n",
    "lost_status_date = np.load(\"data/numpy/lost_status_date.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Function\n",
    "def MSE(test,predicted):\n",
    "    mse = mean_squared_error(test, predicted)\n",
    "    print(\"The mean squared error is {}.\".format(mse))\n",
    "    \n",
    "# Split Dataframe into train and test set\n",
    "def split(dataframe, border, col):\n",
    "    return dataframe.loc[:border,col], dataframe.loc[border:,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all companies we have data for\n",
    "\n",
    "# Get list of file names\n",
    "fileList = os.listdir(\"../data/series/good\")\n",
    "\n",
    "# Loop through file names and collect ticker symbols\n",
    "companyList = []\n",
    "for file in fileList:\n",
    "    companyName = file.split(\"_\")[0]\n",
    "    if companyName not in [\".DS\",\".ipynb\"]:\n",
    "        companyList.append(companyName)\n",
    "print(companyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd634f57-5d41-4cbf-99ff-633e89096bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to ignore list if no data is available\n",
    "# Yahoo Finance will occasionally not through error\n",
    "# for some companies during GetMetrics, which end up creating blank\n",
    "# csvs that we can simply ignore here\n",
    "ignoreList = [\"FFMR\",\"FMCB\"]\n",
    "stockList = list(set(companyList).difference(ignoreList))\n",
    "\n",
    "# Load and store data in initial Dataframe\n",
    "df_ = {}\n",
    "for i in stockList:\n",
    "    df_[i] = pd.read_csv(\"../data/series/good/\" + i + \"_dividends_fixed.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Create new Dataframe that contains data for each company\n",
    "# split at specified year\n",
    "df_new = {}\n",
    "for i in stockList:\n",
    "    df_new[i] = {}\n",
    "    df_new[i][\"Train\"], df_new[i][\"Test\"] = split(df_[i], \"2006\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "transform_train = {}\n",
    "transform_test = {}\n",
    "scaler = {}\n",
    "\n",
    "for num, i in enumerate(stockList):\n",
    "    sc = MinMaxScaler(feature_range = (0,1)) # normalize values to be within [0,1]\n",
    "    train = df_new[i][\"Train\"].values\n",
    "    test = df_new[i][\"Test\"].values\n",
    "    \n",
    "    train = np.expand_dims(train, axis = 1)\n",
    "    test = np.expand_dims(test, axis = 1)\n",
    "    transform_train[i] = sc.fit_transform(train)\n",
    "    transform_test[i] = sc.fit_transform(test)\n",
    "    # Save scalers for predictions at later step\n",
    "    scaler[i] = sc\n",
    "\n",
    "# Free up memory\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically determine which samples have enough information to be trained\n",
    "# Need every dataset to have a similar length for training/testing\n",
    "\n",
    "train_lengths = [data.shape[0] for data in transform_train.values()] \n",
    "val, ct = np.unique(np.array(train_lengths), return_counts=True)\n",
    "train_count = val[np.argsort(ct)][-1]\n",
    "\n",
    "test_lengths = [data.shape[0] for data in transform_test.values()] \n",
    "val, ct = np.unique(np.array(test_lengths), return_counts=True)\n",
    "test_count = val[np.argsort(ct)][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_count -= 20\n",
    "# test_count -= 20\n",
    "train_count, test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many samples will used for prediction\n",
    "# this will use the previous 5 samples\n",
    "overlap = 5\n",
    "\n",
    "trainset = {}\n",
    "testset = {}\n",
    "\n",
    "stockListFix = [] # This will contain stocks that have enough samples to be included in training\n",
    "testDates = {} # collect dates of test samples for later visualizations\n",
    "\n",
    "# Created data structure for input sets and expected outputs\n",
    "# Created by sliding window of len overlap across datasets\n",
    "for j in stockList:\n",
    "        if transform_train[j].shape[0] < train_count or transform_test[j].shape[0] < test_count:\n",
    "            continue\n",
    "        stockListFix.append(j)\n",
    "        trainset[j] = {}\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for i in range(overlap,train_count):\n",
    "            X_train.append(transform_train[j][i-overlap:i,0])\n",
    "            y_train.append(transform_train[j][i,0])\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        trainset[j][\"X\"] = np.expand_dims(X_train, axis=len(X_train.shape))\n",
    "        trainset[j][\"y\"] = y_train\n",
    "\n",
    "        testset[j] = {}\n",
    "        X_test = []\n",
    "        y_test = []   \n",
    "        testDates[j] = []\n",
    "        for i in range(overlap, test_count):\n",
    "            X_test.append(transform_test[j][i-overlap:i,0])\n",
    "            y_test.append(transform_test[j][i,0])\n",
    "            testDates[j].append(df_new[j]['Test'].index[i])\n",
    "        X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "        testset[j][\"X\"] = np.expand_dims(X_test, axis=len(X_test.shape))\n",
    "        testset[j][\"y\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset shapes for each company\n",
    "arr_buff = []\n",
    "for i in stockListFix:\n",
    "    buff = {}\n",
    "    buff[\"X_train\"] = trainset[i][\"X\"].shape\n",
    "    buff[\"y_train\"] = trainset[i][\"y\"].shape\n",
    "    buff[\"X_test\"] = testset[i][\"X\"].shape\n",
    "    buff[\"y_test\"] = testset[i][\"y\"].shape\n",
    "    arr_buff.append(buff)\n",
    "\n",
    "pd.DataFrame(arr_buff, index=stockListFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Examples From Dataset\n",
    "\n",
    "randidx = np.random.permutation(len(stockList))\n",
    "shufStockList = np.array(stockList)[randidx]\n",
    "\n",
    "lost_ct = 0\n",
    "kept_ct = 0\n",
    "\n",
    "lost_figs = [\"PBI\",\"ABT\",\"WRE\",\"DBD\"]\n",
    "kept_figs = [\"CBU\",\"SWK\",\"FUL\",\"AOS\"]\n",
    "\n",
    "lost_fig, lost_axs = plt.subplots(2,2,figsize=(20, 18))\n",
    "lost_fig.suptitle(\"Lost Status Examples\", fontsize=24)\n",
    "\n",
    "kept_fig, kept_axs = plt.subplots(2,2,figsize=(20, 18))        \n",
    "kept_fig.suptitle(\"Kept Status Examples\", fontsize=24)\n",
    "\n",
    "for i in shufStockList:\n",
    "    if df_new[i][\"Train\"].empty or df_new[i][\"Test\"].empty:\n",
    "        print(f\"No Data For {i}, add to ignoreList and rerun cells\")\n",
    "    if i in lost_figs and lost_ct <= 3:\n",
    "        indx = lost_ct\n",
    "        axs = lost_axs.flat\n",
    "        axs[indx].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "#         axs.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "        axs[indx].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))    \n",
    "        axs[indx].plot(df_new[i][\"Train\"].index, df_new[i][\"Train\"].values)\n",
    "        axs[indx].plot(df_new[i][\"Test\"].index,df_new[i][\"Test\"].values)\n",
    "        axs[indx].set_ylabel(\"Dividend\")\n",
    "        axs[indx].set_xlabel(\"Date\")\n",
    "        for label in axs[indx].get_xticklabels(which='major'):\n",
    "            label.set(rotation=90, horizontalalignment='right')\n",
    "        axs[indx].legend([\"Training Set\", \"Test Set\"])  \n",
    "        dateIdx = [i in lost_status_date[:,2][j] for j in range(len(lost_status_date))]\n",
    "        date = lost_status_date[dateIdx][0][:2]        \n",
    "        axs[indx].set_title(i + f\" Dividend Values - Lost Status on {date[1]} {date[0]}\")   \n",
    "        # plt.savefig(\"WRE_Dataset.png\", format=\"png\")\n",
    "        lost_ct += 1\n",
    "        \n",
    "    if i in kept_figs and kept_ct <= 3:\n",
    "        indx = kept_ct\n",
    "        axs = kept_axs.flat\n",
    "        axs[indx].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "#         axs.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "        axs[indx].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))    \n",
    "        axs[indx].plot(df_new[i][\"Train\"].index, df_new[i][\"Train\"].values)\n",
    "        axs[indx].plot(df_new[i][\"Test\"].index,df_new[i][\"Test\"].values)\n",
    "        axs[indx].set_ylabel(\"Dividend\")\n",
    "        axs[indx].set_xlabel(\"Date\")\n",
    "        for label in axs[indx].get_xticklabels(which='major'):\n",
    "            label.set(rotation=90, horizontalalignment='right')\n",
    "        axs[indx].legend([\"Training Set\", \"Test Set\"])\n",
    "        axs[indx].set_title(i + \" Dividend Value\") \n",
    "        # plt.savefig(\"WRE_Dataset.png\", format=\"png\")\n",
    "        kept_ct += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccbdd1-5763-42a4-bdc9-97fb3bb8d242",
   "metadata": {},
   "source": [
    "#### NOTE: Some companies in the \"lost status\" category could have lost status without their being a significant dip in their historical dividend values. For this project, we are focusing on attempting to predict the next dividend value even if it has a sudden decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-chinese",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create LSTM Model\n",
    "\n",
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularization\n",
    "regressor.add(InputLayer(input_shape=(X_train.shape[1],1), name = \"Model_Input_Layer\"))\n",
    "regressor.add(Bidirectional(LSTM(units=50, return_sequences=True, name=\"LSTM_Layer_1\")))\n",
    "regressor.add(Dropout(0.2,name=\"Dropout_Layer_1\"))\n",
    "# Second LSTM layer\n",
    "regressor.add(Bidirectional(LSTM(units=50, return_sequences=True,name=\"LSTM_Layer_2\")))\n",
    "regressor.add(Dropout(0.3,name=\"Dropout_Layer_2\"))\n",
    "# Third LSTM layer\n",
    "regressor.add(Bidirectional(LSTM(units=50, return_sequences=True,name=\"LSTM_Layer_3\")))\n",
    "regressor.add(Dropout(0.3,name=\"Dropout_Layer_3\"))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50, name=\"LSTM_Layer_4\"))\n",
    "regressor.add(Dropout(0.5,name=\"Dropout_Layer_4\"))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1, name=\"Dense_Output_Layer\")) \n",
    "\n",
    "plot_model(\n",
    "    regressor,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-title",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Fitting to the training set\n",
    "hists = {}\n",
    "for i in stockListFix:\n",
    "    print(\"Fitting to\", i)\n",
    "    hists[i] = regressor.fit(trainset[i][\"X\"], trainset[i][\"y\"], epochs=10, batch_size=5,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = {}\n",
    "\n",
    "lost_ct = 0\n",
    "kept_ct = 0\n",
    "\n",
    "\n",
    "lost_fig, lost_axs = plt.subplots(2,2,figsize=(20, 18))        \n",
    "lost_fig.suptitle(\"Lost Status Examples\", fontsize=24)\n",
    "\n",
    "kept_fig, kept_axs = plt.subplots(2,2,figsize=(20, 18))        \n",
    "kept_fig.suptitle(\"Kept Status Examples\", fontsize=24)\n",
    "\n",
    "for i in stockListFix:\n",
    "    # if i in shufStockList:\n",
    "        y_true = scaler[i].inverse_transform(testset[i][\"y\"].reshape(-1,1))\n",
    "        y_pred = scaler[i].inverse_transform(regressor.predict(testset[i][\"X\"]))\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        pred_result[i] = {}\n",
    "        pred_result[i][\"True\"] = y_true\n",
    "        pred_result[i][\"Pred\"] = y_pred\n",
    "\n",
    "        if i in lost_figs and lost_ct <= 3:\n",
    "            indx = lost_ct\n",
    "            axs = lost_axs.flat\n",
    "            axs[indx].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "            axs[indx].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            axs[indx].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))    \n",
    "            axs[indx].plot(pd.DatetimeIndex(testDates[i]), y_true)\n",
    "            axs[indx].plot(pd.DatetimeIndex(testDates[i]),y_pred)\n",
    "            axs[indx].set_ylabel(\"Dividend\")\n",
    "            axs[indx].set_xlabel(\"Date\")\n",
    "            for label in axs[indx].get_xticklabels(which='major'):\n",
    "                label.set(rotation=90, horizontalalignment='right')\n",
    "            axs[indx].legend([\"Actual\", \"Predicted\"])\n",
    "            axs[indx].set_title(\"{} with MSE {:10.4f}\".format(i,mse))\n",
    "            lost_ct += 1\n",
    "\n",
    "        if i in kept_figs and kept_ct <= 3:\n",
    "            indx = kept_ct\n",
    "            axs = kept_axs.flat\n",
    "            axs[indx].xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1)))\n",
    "            axs[indx].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "            axs[indx].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))    \n",
    "            axs[indx].plot(pd.DatetimeIndex(testDates[i]), y_true)\n",
    "            axs[indx].plot(pd.DatetimeIndex(testDates[i]),y_pred)\n",
    "            axs[indx].set_ylabel(\"Dividend\")\n",
    "            axs[indx].set_xlabel(\"Date\")\n",
    "            for label in axs[indx].get_xticklabels(which='major'):\n",
    "                label.set(rotation=90, horizontalalignment='right')\n",
    "            axs[indx].legend([\"Actual\", \"Predicted\"])\n",
    "            axs[indx].set_title(\"{} with MSE {:10.4f}\".format(i,mse))\n",
    "            kept_ct += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedd522-0844-481d-93c9-64a9d7508d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler[\"BMS\"].inverse_transform(testset[\"BMS\"][\"y\"].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0069d-c452-406b-bf97-06faf96705ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset[\"BMS\"][\"y\"],df_new[\"BMS\"][\"Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = df_new[\"WRE\"][\"Test\"][overlap:].index\n",
    "def lagging(df, lag, time_index):\n",
    "    df_pred = pd.Series(df[\"Pred\"].reshape(-1), index=time_index)\n",
    "    df_true = pd.Series(df[\"True\"].reshape(-1), index=time_index)\n",
    "    \n",
    "    df_pred_lag = df_pred.shift(lag)\n",
    "    \n",
    "    print(\"MSE without Lag\", mean_squared_error(np.array(df_true), np.array(df_pred)))\n",
    "    print(\"MSE with Lag 5\", mean_squared_error(np.array(df_true[:lag]), np.array(df_pred_lag[:lag])))\n",
    "\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10, 7))\n",
    "    axs.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "    axs.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    axs.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))  \n",
    "    axs.plot(df_true)\n",
    "    axs.plot(df_pred)\n",
    "    axs.set_ylabel(\"Dividend\")\n",
    "    axs.set_xlabel(\"Date\")\n",
    "    for label in axs.get_xticklabels(which='major'):\n",
    "        label.set(rotation=90, horizontalalignment='right')\n",
    "\n",
    "    axs.legend([\"Actual\", \"Predicted\"])\n",
    "    axs.set_title(\"Prediction\")\n",
    "    plt.show()\n",
    "\n",
    "    MSE_lag = mean_squared_error(np.array(df_true[:lag]), np.array(df_pred_lag[:lag]))\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10, 7))\n",
    "    axs.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "    axs.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    axs.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))  \n",
    "    axs.plot(df_true)\n",
    "    axs.plot(df_pred_lag)\n",
    "    axs.set_ylabel(\"Dividend\")\n",
    "    axs.set_xlabel(\"Date\")\n",
    "    for label in axs.get_xticklabels(which='major'):\n",
    "        label.set(rotation=90, horizontalalignment='right')\n",
    "\n",
    "    axs.legend([\"Actual\", \"Predicted\"])\n",
    "    axs.set_title(\"{} (Lag Adjusted) with MSE {:10.4f}\".format(\"WRE\",MSE_lag))\n",
    "    plt.savefig(\"WRE_Prediction_Lag.png\", format=\"png\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagging(pred_result[\"BMS\"], -1, time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-aquarium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
